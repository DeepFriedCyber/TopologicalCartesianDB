#!/usr/bin/env python3
"""
Persistent Homology Mathematical Model

Revolutionary addition to Multi-Cube Mathematical Evolution system.
Provides multi-scale topological analysis with persistence diagrams.

Key Features:
- Multi-scale topological feature detection
- Persistence diagram generation
- Bottleneck distance computation
- Noise-robust analysis
- Birth-death feature tracking
"""

import numpy as np
from typing import Dict, List, Tuple, Any, Optional
import logging
from dataclasses import dataclass
import time

try:
    import gudhi as gd
    GUDHI_AVAILABLE = True
except ImportError:
    GUDHI_AVAILABLE = False
    logging.warning("GUDHI not available - using simplified persistent homology")

try:
    from sklearn.metrics import pairwise_distances
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class PersistenceDiagram:
    """Persistence diagram representation"""
    dimension: int
    birth_death_pairs: List[Tuple[float, float]]
    features: List[Dict[str, Any]]
    bottleneck_distance: Optional[float] = None
    
    def __post_init__(self):
        """Compute additional metrics"""
        if self.birth_death_pairs:
            self.persistence_values = [death - birth for birth, death in self.birth_death_pairs]
            self.max_persistence = max(self.persistence_values) if self.persistence_values else 0.0
            self.total_persistence = sum(self.persistence_values)
            self.num_features = len(self.birth_death_pairs)
        else:
            self.persistence_values = []
            self.max_persistence = 0.0
            self.total_persistence = 0.0
            self.num_features = 0

@dataclass
class PersistentHomologyResult:
    """Complete persistent homology analysis result"""
    diagrams: Dict[int, PersistenceDiagram]  # dimension -> diagram
    betti_numbers: Dict[int, int]
    total_persistence: float
    max_persistence: float
    stability_score: float
    computation_time: float
    
    def get_feature_vector(self) -> np.ndarray:
        """Extract feature vector for ML applications"""
        features = []
        
        # Betti numbers for dimensions 0, 1, 2
        for dim in range(3):
            features.append(self.betti_numbers.get(dim, 0))
        
        # Persistence statistics
        features.extend([
            self.total_persistence,
            self.max_persistence,
            self.stability_score
        ])
        
        # Per-dimension persistence statistics
        for dim in range(3):
            if dim in self.diagrams:
                diagram = self.diagrams[dim]
                features.extend([
                    diagram.max_persistence,
                    diagram.total_persistence,
                    diagram.num_features
                ])
            else:
                features.extend([0.0, 0.0, 0])
        
        return np.array(features, dtype=np.float32)

class PersistentHomologyModel:
    """
    Persistent Homology Mathematical Model
    
    Analyzes topological features across multiple scales using persistent homology.
    Provides robust, multi-scale analysis perfect for information retrieval.
    """
    
    def __init__(self, 
                 max_dimension: int = 2,
                 max_edge_length: float = 1.0,
                 num_points_threshold: int = 1000,
                 use_rips_complex: bool = True):
        """
        Initialize Persistent Homology Model
        
        Args:
            max_dimension: Maximum homology dimension to compute
            max_edge_length: Maximum edge length for complex construction
            num_points_threshold: Threshold for switching to approximate methods
            use_rips_complex: Whether to use Rips complex (vs Alpha complex)
        """
        self.max_dimension = max_dimension
        self.max_edge_length = max_edge_length
        self.num_points_threshold = num_points_threshold
        self.use_rips_complex = use_rips_complex
        
        self.scaler = StandardScaler() if SKLEARN_AVAILABLE else None
        
        logger.info(f"üî¨ PersistentHomologyModel initialized")
        logger.info(f"   Max dimension: {max_dimension}")
        logger.info(f"   GUDHI available: {GUDHI_AVAILABLE}")
        logger.info(f"   Complex type: {'Rips' if use_rips_complex else 'Alpha'}")
    
    def compute_persistent_homology(self, coordinates: np.ndarray) -> PersistentHomologyResult:
        """
        Compute persistent homology of coordinate data
        
        Args:
            coordinates: Input coordinate data (n_points, n_dimensions)
            
        Returns:
            PersistentHomologyResult with complete analysis
        """
        start_time = time.time()
        
        try:
            if GUDHI_AVAILABLE and coordinates.shape[0] <= self.num_points_threshold:
                result = self._compute_gudhi_persistent_homology(coordinates)
            else:
                result = self._compute_simplified_persistent_homology(coordinates)
            
            computation_time = time.time() - start_time
            result.computation_time = computation_time
            
            logger.info(f"‚úÖ Persistent homology computed in {computation_time:.3f}s")
            logger.info(f"   Betti numbers: {result.betti_numbers}")
            logger.info(f"   Max persistence: {result.max_persistence:.3f}")
            
            return result
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Persistent homology computation failed: {e}")
            return self._create_fallback_result(coordinates, time.time() - start_time)
    
    def _compute_gudhi_persistent_homology(self, coordinates: np.ndarray) -> PersistentHomologyResult:
        """Compute persistent homology using GUDHI library"""
        
        # Normalize coordinates
        if self.scaler and SKLEARN_AVAILABLE:
            coords_normalized = self.scaler.fit_transform(coordinates)
        else:
            coords_normalized = coordinates
        
        if self.use_rips_complex:
            # Vietoris-Rips complex
            rips_complex = gd.RipsComplex(points=coords_normalized, 
                                        max_edge_length=self.max_edge_length)
            simplex_tree = rips_complex.create_simplex_tree(max_dimension=self.max_dimension)
        else:
            # Alpha complex (for low-dimensional data)
            if coords_normalized.shape[1] <= 3:
                alpha_complex = gd.AlphaComplex(points=coords_normalized)
                simplex_tree = alpha_complex.create_simplex_tree()
            else:
                # Fall back to Rips for high-dimensional data
                rips_complex = gd.RipsComplex(points=coords_normalized, 
                                            max_edge_length=self.max_edge_length)
                simplex_tree = rips_complex.create_simplex_tree(max_dimension=self.max_dimension)
        
        # Compute persistent homology
        persistence = simplex_tree.persistence()
        
        # Process results
        diagrams = {}
        betti_numbers = {}
        
        for dim in range(self.max_dimension + 1):
            # Extract persistence pairs for this dimension
            pairs = [(birth, death) for (dimension, (birth, death)) in persistence 
                    if dimension == dim and death != float('inf')]
            
            # Count infinite persistence (Betti numbers)
            infinite_pairs = [(birth, float('inf')) for (dimension, (birth, death)) in persistence 
                            if dimension == dim and death == float('inf')]
            
            betti_numbers[dim] = len(infinite_pairs)
            
            # Create persistence diagram
            if pairs or infinite_pairs:
                all_pairs = pairs + [(birth, birth + self.max_edge_length) for birth, _ in infinite_pairs]
                features = [{'birth': birth, 'death': death, 'persistence': death - birth} 
                          for birth, death in all_pairs]
                
                diagrams[dim] = PersistenceDiagram(
                    dimension=dim,
                    birth_death_pairs=all_pairs,
                    features=features
                )
        
        # Compute overall statistics
        total_persistence = sum(diagram.total_persistence for diagram in diagrams.values())
        max_persistence = max([diagram.max_persistence for diagram in diagrams.values()] + [0.0])
        
        # Stability score (simplified)
        stability_score = min(1.0, max_persistence / (self.max_edge_length + 1e-8))
        
        return PersistentHomologyResult(
            diagrams=diagrams,
            betti_numbers=betti_numbers,
            total_persistence=total_persistence,
            max_persistence=max_persistence,
            stability_score=stability_score,
            computation_time=0.0  # Will be set by caller
        )
    
    def _compute_simplified_persistent_homology(self, coordinates: np.ndarray) -> PersistentHomologyResult:
        """Simplified persistent homology for when GUDHI is not available"""
        
        # Compute pairwise distances
        if SKLEARN_AVAILABLE:
            distances = pairwise_distances(coordinates)
        else:
            distances = np.linalg.norm(coordinates[:, np.newaxis] - coordinates[np.newaxis, :], axis=2)
        
        # Simple clustering-based approach
        diagrams = {}
        betti_numbers = {}
        
        # Dimension 0: Connected components
        threshold_values = np.linspace(0, self.max_edge_length, 20)
        birth_death_pairs = []
        
        for i, threshold in enumerate(threshold_values[:-1]):
            next_threshold = threshold_values[i + 1]
            
            # Count connected components at this threshold
            adjacency = distances <= threshold
            components = self._count_connected_components(adjacency)
            next_adjacency = distances <= next_threshold
            next_components = self._count_connected_components(next_adjacency)
            
            # Components that die
            dying_components = components - next_components
            for _ in range(dying_components):
                birth_death_pairs.append((0.0, threshold))
        
        # Create dimension 0 diagram
        if birth_death_pairs:
            features = [{'birth': birth, 'death': death, 'persistence': death - birth} 
                       for birth, death in birth_death_pairs]
            diagrams[0] = PersistenceDiagram(
                dimension=0,
                birth_death_pairs=birth_death_pairs,
                features=features
            )
        
        # Final connected components (infinite persistence)
        final_adjacency = distances <= self.max_edge_length
        final_components = self._count_connected_components(final_adjacency)
        betti_numbers[0] = final_components
        
        # Higher dimensions (simplified)
        for dim in range(1, self.max_dimension + 1):
            betti_numbers[dim] = 0  # Simplified: assume no higher-dimensional holes
        
        # Compute statistics
        total_persistence = sum(diagram.total_persistence for diagram in diagrams.values())
        max_persistence = max([diagram.max_persistence for diagram in diagrams.values()] + [0.0])
        stability_score = min(1.0, max_persistence / (self.max_edge_length + 1e-8))
        
        return PersistentHomologyResult(
            diagrams=diagrams,
            betti_numbers=betti_numbers,
            total_persistence=total_persistence,
            max_persistence=max_persistence,
            stability_score=stability_score,
            computation_time=0.0
        )
    
    def _count_connected_components(self, adjacency_matrix: np.ndarray) -> int:
        """Count connected components in adjacency matrix using DFS"""
        n = adjacency_matrix.shape[0]
        visited = np.zeros(n, dtype=bool)
        components = 0
        
        def dfs(node):
            visited[node] = True
            for neighbor in range(n):
                if adjacency_matrix[node, neighbor] and not visited[neighbor]:
                    dfs(neighbor)
        
        for i in range(n):
            if not visited[i]:
                dfs(i)
                components += 1
        
        return components
    
    def _create_fallback_result(self, coordinates: np.ndarray, computation_time: float) -> PersistentHomologyResult:
        """Create fallback result when computation fails"""
        
        # Very basic analysis
        n_points = coordinates.shape[0]
        
        return PersistentHomologyResult(
            diagrams={},
            betti_numbers={0: 1, 1: 0, 2: 0},  # Assume single connected component
            total_persistence=0.0,
            max_persistence=0.0,
            stability_score=0.0,
            computation_time=computation_time
        )
    
    def compute_similarity_score(self, coordinates1: np.ndarray, coordinates2: np.ndarray) -> float:
        """
        Compute similarity between two coordinate sets using persistent homology
        
        Args:
            coordinates1: First coordinate set
            coordinates2: Second coordinate set
            
        Returns:
            Similarity score between 0 and 1
        """
        try:
            # Compute persistent homology for both
            ph1 = self.compute_persistent_homology(coordinates1)
            ph2 = self.compute_persistent_homology(coordinates2)
            
            # Compare feature vectors
            features1 = ph1.get_feature_vector()
            features2 = ph2.get_feature_vector()
            
            # Compute similarity (inverse of normalized distance)
            distance = np.linalg.norm(features1 - features2)
            max_distance = np.linalg.norm(features1) + np.linalg.norm(features2) + 1e-8
            similarity = 1.0 - (distance / max_distance)
            
            return max(0.0, min(1.0, similarity))
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Persistent homology similarity computation failed: {e}")
            return 0.0
    
    def analyze_stability(self, coordinates: np.ndarray, noise_levels: List[float] = None) -> Dict[str, float]:
        """
        Analyze stability of persistent homology under noise
        
        Args:
            coordinates: Input coordinates
            noise_levels: List of noise levels to test
            
        Returns:
            Stability analysis results
        """
        if noise_levels is None:
            noise_levels = [0.01, 0.05, 0.1, 0.2]
        
        # Compute baseline
        baseline_ph = self.compute_persistent_homology(coordinates)
        baseline_features = baseline_ph.get_feature_vector()
        
        stability_scores = []
        
        for noise_level in noise_levels:
            # Add noise
            noise = np.random.normal(0, noise_level, coordinates.shape)
            noisy_coordinates = coordinates + noise
            
            # Compute persistent homology
            noisy_ph = self.compute_persistent_homology(noisy_coordinates)
            noisy_features = noisy_ph.get_feature_vector()
            
            # Compute stability
            distance = np.linalg.norm(baseline_features - noisy_features)
            max_distance = np.linalg.norm(baseline_features) + np.linalg.norm(noisy_features) + 1e-8
            stability = 1.0 - (distance / max_distance)
            stability_scores.append(max(0.0, stability))
        
        return {
            'noise_levels': noise_levels,
            'stability_scores': stability_scores,
            'average_stability': np.mean(stability_scores),
            'min_stability': np.min(stability_scores),
            'stability_variance': np.var(stability_scores)
        }

# Integration with existing mathematical model system
def create_persistent_homology_experiment(coordinates: np.ndarray, 
                                        cube_name: str,
                                        parameters: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    Create persistent homology experiment for Multi-Cube Mathematical Evolution
    
    Args:
        coordinates: Input coordinate data
        cube_name: Name of the cube running the experiment
        parameters: Optional parameters for the model
        
    Returns:
        Experiment result dictionary
    """
    if parameters is None:
        parameters = {}
    
    # Initialize model with parameters
    model = PersistentHomologyModel(
        max_dimension=parameters.get('max_dimension', 2),
        max_edge_length=parameters.get('max_edge_length', 1.0),
        num_points_threshold=parameters.get('num_points_threshold', 1000),
        use_rips_complex=parameters.get('use_rips_complex', True)
    )
    
    start_time = time.time()
    
    try:
        # Compute persistent homology
        ph_result = model.compute_persistent_homology(coordinates)
        
        # Extract performance metrics
        feature_vector = ph_result.get_feature_vector()
        
        # Compute improvement score (based on topological richness)
        improvement_score = (
            ph_result.stability_score * 0.4 +
            min(1.0, ph_result.total_persistence / 10.0) * 0.3 +
            min(1.0, sum(ph_result.betti_numbers.values()) / 10.0) * 0.3
        )
        
        execution_time = time.time() - start_time
        
        return {
            'model_type': 'persistent_homology',
            'cube_name': cube_name,
            'success': True,
            'improvement_score': improvement_score,
            'execution_time': execution_time,
            'performance_metrics': {
                'betti_numbers': ph_result.betti_numbers,
                'total_persistence': ph_result.total_persistence,
                'max_persistence': ph_result.max_persistence,
                'stability_score': ph_result.stability_score,
                'feature_vector_norm': np.linalg.norm(feature_vector)
            },
            'parameters': parameters,
            'detailed_result': ph_result
        }
        
    except Exception as e:
        execution_time = time.time() - start_time
        logger.warning(f"‚ö†Ô∏è Persistent homology experiment failed: {e}")
        
        return {
            'model_type': 'persistent_homology',
            'cube_name': cube_name,
            'success': False,
            'improvement_score': 0.0,
            'execution_time': execution_time,
            'error_message': str(e),
            'parameters': parameters
        }

if __name__ == "__main__":
    # Test the persistent homology model
    print("üî¨ Testing Persistent Homology Model")
    
    # Generate test data
    np.random.seed(42)
    test_coordinates = np.random.rand(50, 3)
    
    # Test basic functionality
    model = PersistentHomologyModel()
    result = model.compute_persistent_homology(test_coordinates)
    
    print(f"‚úÖ Persistent homology computed successfully")
    print(f"   Betti numbers: {result.betti_numbers}")
    print(f"   Total persistence: {result.total_persistence:.3f}")
    print(f"   Max persistence: {result.max_persistence:.3f}")
    print(f"   Stability score: {result.stability_score:.3f}")
    
    # Test experiment integration
    experiment_result = create_persistent_homology_experiment(test_coordinates, "test_cube")
    print(f"‚úÖ Experiment integration successful")
    print(f"   Improvement score: {experiment_result['improvement_score']:.3f}")
    print(f"   Execution time: {experiment_result['execution_time']:.3f}s")